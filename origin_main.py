"""
main.py  - 2025-05-31
åƒåœ¾æ–‡æœ¬æ£€æµ‹ï¼šå•/äº¤å‰æ•°æ®é›†ä¸¤ç§æ¨¡å¼
========================================================
# MODE = 1  â†’ â€œå•æ•°æ®é›†æ¨¡å¼â€ï¼šå¯¹ SINGLE_DATA åš 50/50 åˆ’åˆ†
# MODE = 2  â†’ â€œäº¤å‰æ•°æ®é›†æ¨¡å¼â€ï¼šç”¨ TRAIN_DATA è®­ç»ƒï¼Œç”¨ TEST_DATA æµ‹è¯•
========================================================
ä¾èµ–ï¼š
  pip install numpy gensim scikit-learn imbalanced-learn tqdm
"""

import os, re, sys, logging
from typing import List, Tuple

import numpy as np
from tqdm import tqdm
import time
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import RandomOverSampler
from gensim.models import Word2Vec

# ---------- é¡¹ç›®è‡ªå¸¦å·¥å…·å‡½æ•° ----------
from utils import (
    count_chinese_characters, load_chinese_characters,
    compute_sim_mat,      load_sim_mat
)

# ---------------- å…¨å±€é…ç½® ----------------
DEFAULT_DATA_DIR = 'data'          # ç»Ÿä¸€çš„æ•°æ®æ ¹ç›®å½•
RES_DIR           = 'res'          # ç¼“å­˜ç›®å½•
os.makedirs(RES_DIR, exist_ok=True)

# MODE = 1 è¡¨ç¤ºâ€œå•æ•°æ®é›†æ¨¡å¼â€ï¼šå¯¹ SINGLE_DATA åš 50/50 åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•
# MODE = 2 è¡¨ç¤ºâ€œäº¤å‰æ•°æ®é›†æ¨¡å¼â€ï¼šç”¨ TRAIN_DATA è®­ç»ƒï¼Œç”¨ TEST_DATA æµ‹è¯•
MODE = 1

# å¦‚æœ MODE == 1ï¼Œè„šæœ¬ä¼šä½¿ç”¨ SINGLE_DATA åšåˆ’åˆ†
SINGLE_DATA = 'big_dataset.txt'

# å¦‚æœ MODE == 2ï¼Œè„šæœ¬ä¼šç”¨ TRAIN_DATA åšè®­ç»ƒï¼Œç”¨ TEST_DATA åšæµ‹è¯•
TRAIN_DATA = 'big_dataset.txt'
TEST_DATA = 'dataset.txt'

# ---------------------------------------------------------------

# æ—¥å¿—ï¼šINFO çº§åˆ«å³å¯ï¼›å¯æ”¹ä¸º DEBUG çœ‹æ›´ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)

# ---------------- åŸºç¡€ I/O ----------------
def read_data(filename: str) -> Tuple[List[str], List[str]]:
    """
    è¯»å–åŸå§‹è¯­æ–™ï¼Œæ¯è¡Œæ ¼å¼ï¼š 'æ ‡ç­¾<TAB>æ–‡æœ¬'
    è¿”å› (tag_list, text_list)
    """
    logging.info('è¯»å–æ•°æ®: %s', filename)
    if not os.path.isfile(filename):
        logging.error('âŒ æ–‡ä»¶ä¸å­˜åœ¨: %s', filename)
        sys.exit(1)

    with open(filename, 'r', encoding='utf-8') as f:
        raw = f.readlines()

    dataset = [s.strip().split('\t', 1) for s in raw]
    dataset = [d for d in dataset if len(d) == 2 and d[1].strip()]
    if not dataset:
        logging.error('âŒ æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼ä¸ç¬¦: %s', filename)
        sys.exit(1)

    tag, text = zip(*dataset)
    logging.info('âœ”ï¸ å…±è¯»å– %d è¡Œæ ·æœ¬', len(text))
    return list(tag), list(text)


# ---------------- æ–‡æœ¬é¢„å¤„ç† ----------------
def clean_text(dataset: List[str]) -> List[str]:
    """
    æ¸…æ´—ï¼šä¿ç•™ä¸­æ–‡ / è‹±æ–‡ / æ•°å­— / ç©ºæ ¼
    """
    cleaned = []
    for line in tqdm(dataset, desc='Cleaning text', ncols=80):
        cleaned.append(re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', line).strip())
    return cleaned


def tokenize_and_remove_stopwords(dataset: List[str]) -> List[str]:
    """
    é€å­—ç¬¦â€œåˆ†è¯â€ï¼Œä»…ä¿ç•™ä¸­æ–‡ï¼Œä¸”ä¸åœ¨åœç”¨è¯è¡¨ä¸­çš„å­—ç¬¦
    æœªæ‰¾åˆ°åœç”¨è¯æ–‡ä»¶æ—¶ç»™å‡ºè­¦å‘Šå¹¶ç»§ç»­
    """
    stop_file = os.path.join(DEFAULT_DATA_DIR, 'hit_stopwords.txt')
    if os.path.isfile(stop_file):
        with open(stop_file, 'r', encoding='utf-8') as f:
            stopwords = {s.strip() for s in f}
        logging.info('âœ”ï¸ åœç”¨è¯åŠ è½½å®Œæˆï¼Œæ•°é‡=%d', len(stopwords))
    else:
        stopwords = set()
        logging.warning('âš ï¸  æœªæ‰¾åˆ°åœç”¨è¯æ–‡ä»¶ %sï¼Œè·³è¿‡åœç”¨', stop_file)

    tokenized = []
    for line in tqdm(dataset, desc='Tokenizing / filtering', ncols=80):
        tokenized.append(''.join(
            [c for c in line if (c not in stopwords and re.search('[\u4e00-\u9fa5]', c))]
        ))
    return tokenized


# ---------------- Word2Vec ----------------
def train_w2v(sentences: List[str], d: int = 100) -> dict:
    """
    ä½¿ç”¨ gensim Word2Vec è®­ç»ƒå­—å‘é‡ï¼›è¿”å› {å­—:å‘é‡} å­—å…¸
    """
    model = Word2Vec(
        sentences=sentences,
        vector_size=d,
        window=5,
        min_count=1,
        sg=0,
        workers=os.cpu_count()
    )
    w2v = {}
    for sent in tqdm(sentences, desc='Collecting vectors', ncols=80):
        for ch in sent:
            if ch not in w2v:
                w2v[ch] = model.wv[ch]
    logging.info('âœ”ï¸ Word2Vec è®­ç»ƒå®Œæˆï¼Œå­—è¡¨å¤§å°=%d', len(w2v))
    return w2v


# ---------------- åŠ æƒå­—å‘é‡ ----------------
def generate_char_vectors(
    chinese_chars: List[str],
    w2v: dict,
    sim_mat: np.ndarray,
    char_count: dict,
    threshold: float = 0.6
) -> dict:
    """
    è®ºæ–‡é‡Œçš„åŠ æƒå­—å‘é‡ï¼›è‹¥æŸå­—ç¬¦åœ¨ w2v ä¸­ä¸å­˜åœ¨ â†’ ç›´æ¥è·³è¿‡
    """
    ref_vec = next(iter(w2v.values()))
    d = ref_vec.shape[0]
    char_vec = {}

    for i in tqdm(range(len(chinese_chars)), desc='Generating char vectors', ncols=80):
        ch = chinese_chars[i]
        if ch not in w2v:
            continue  # OOV å­—ç¬¦ç›´æ¥å¿½ç•¥

        # æ‰¾ç›¸ä¼¼å­—ç¬¦
        group = [ch2 for j, ch2 in enumerate(chinese_chars) if sim_mat[i][j] >= threshold]
        total = 0
        emb   = np.zeros(d, dtype=np.float32)
        for g in group:
            if g not in w2v:
                continue
            emb   += char_count.get(g, 1) * w2v[g]
            total += char_count.get(g, 1)
        char_vec[ch] = emb / (total if total else 1)

    logging.info('âœ”ï¸ åŠ æƒå­—å‘é‡ç”Ÿæˆå®Œæˆï¼Œå¤§å°=%d', len(char_vec))
    return char_vec


# ---------------- å¥å‘é‡ ----------------
def generate_sentence_vectors(texts: List[str], char_vec: dict, d: int = 100) -> List[np.ndarray]:
    """
    åŠ¨æ€è·¯ç”±å¥å‘é‡ã€‚å¯¹ char_vec ä¸­ç¼ºå¤±çš„å­—ç¬¦ç›´æ¥è·³è¿‡
    """
    sent_vecs = []
    for sent in tqdm(texts, desc='Generating sentence vectors', ncols=80):
        chars = [c for c in sent if c in char_vec]
        if not chars:                            # æ•´å¥æ— åˆæ³•å­—ç¬¦
            sent_vecs.append(np.zeros(d, dtype=np.float32))
            continue

        n = len(chars)
        alpha = np.zeros((n, n), dtype=np.float32)
        for i in range(n):
            for j in range(n):
                alpha[i, j] = np.dot(char_vec[chars[i]], char_vec[chars[j]]) / np.sqrt(d)

        # softmax
        alpha_hat = np.exp(alpha) / np.sum(np.exp(alpha), axis=1, keepdims=True)

        m = np.zeros(d, dtype=np.float32)
        for i in range(n):
            mi = np.zeros(d, dtype=np.float32)
            for j in range(n):
                mi += alpha_hat[i, j] * char_vec[chars[j]]
            m += mi
        sent_vecs.append(m / d)
    return sent_vecs


# ---------------- åˆ†ç±» / è¯„ä¼° ----------------
def spam_classification(train_y, train_X, test_X):
    logreg = LogisticRegression(max_iter=200)
    logreg.fit(np.asarray(train_X), np.asarray(train_y))
    return logreg.predict(test_X)


def evaluate(true_y, pred_y):
    logging.info('\næ··æ·†çŸ©é˜µï¼š\n%s', confusion_matrix(true_y, pred_y))
    logging.info('\nåˆ†ç±»æŠ¥å‘Šï¼š\n%s', classification_report(true_y, pred_y, digits=3))


# ---------------- ä¸»ç¨‹åº ----------------
def main():
    start_time = time.time()
    # --------------------------------------------------
    # 1) æ•°æ®åŠ è½½
    # --------------------------------------------------
    if MODE == 1:
        # å•æ•°æ®é›† âœ 50/50
        file_path = os.path.join(DEFAULT_DATA_DIR, SINGLE_DATA)
        tags, texts = read_data(file_path)
        tags_train, tags_test, texts_train, texts_test = train_test_split(
            tags, texts, test_size=0.5, random_state=42, stratify=tags
        )
    else:
        # äº¤å‰æ•°æ®é›†
        tr_path = os.path.join(DEFAULT_DATA_DIR, TRAIN_DATA)
        te_path = os.path.join(DEFAULT_DATA_DIR, TEST_DATA)
        tags_train, texts_train = read_data(tr_path)
        tags_test,  texts_test  = read_data(te_path)

    logging.info('è®­ç»ƒæ ·æœ¬=%dï¼Œæµ‹è¯•æ ·æœ¬=%d', len(texts_train), len(texts_test))

    # --------------------------------------------------
    # 2) æ–‡æœ¬æ¸…æ´— & â€œåˆ†è¯â€
    # --------------------------------------------------
    clean_train = clean_text(texts_train)
    token_train = tokenize_and_remove_stopwords(clean_train)

    clean_test  = clean_text(texts_test)
    token_test  = tokenize_and_remove_stopwords(clean_test)

    # --------------------------------------------------
    # 3) æ±‰å­—ç»Ÿè®¡ / å£°å½¢ç›¸ä¼¼åº¦
    #    ä»…åŸºäºã€è®­ç»ƒé›†ã€‘ç»Ÿè®¡ï¼Œæµ‹è¯•é›†å‡ºç°çš„ OOV ä¼šè¢«å¿½ç•¥
    # --------------------------------------------------
    HANZI_CACHE = os.path.join(RES_DIR, 'hanzi.txt')
    SIM_CACHE   = os.path.join(RES_DIR, 'similarity_matrix.pkl')

    if os.path.isfile(HANZI_CACHE):
        chinese_chars, char_count, char_code = load_chinese_characters(HANZI_CACHE)
        logging.info('âœ”ï¸ ä»ç¼“å­˜åŠ è½½æ±‰å­—è¡¨ï¼Œå¤§å°=%d', len(chinese_chars))
    else:
        chinese_chars, char_count, char_code = count_chinese_characters(
            texts_train, HANZI_CACHE)
        logging.info('âœ”ï¸ é‡æ–°ç»Ÿè®¡æ±‰å­—è¡¨å¹¶å†™å…¥ç¼“å­˜')

    # ç»Ÿä¸€æŠŠé¢‘æ¬¡è½¬ int
    char_count = {c: int(n) for c, n in char_count.items()}

    if os.path.isfile(SIM_CACHE):
        sim_mat = load_sim_mat(SIM_CACHE)
        logging.info('âœ”ï¸ ä»ç¼“å­˜åŠ è½½å£°å½¢ç›¸ä¼¼åº¦çŸ©é˜µ')
    else:
        sim_mat = compute_sim_mat(chinese_chars, char_code)
        logging.info('âœ”ï¸ é‡æ–°è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µå¹¶å†™å…¥ç¼“å­˜')

    # --------------------------------------------------
    # 4) Word2Vec & å‘é‡æ„å»º
    # --------------------------------------------------
    w2v = train_w2v(token_train, d=100)
    char_vec = generate_char_vectors(chinese_chars, w2v, sim_mat, char_count, threshold=0.6)

    sent_vec_train = generate_sentence_vectors(token_train, char_vec, d=100)
    sent_vec_test  = generate_sentence_vectors(token_test,  char_vec, d=100)

    # --------------------------------------------------
    # 5) è®­ç»ƒ + é¢„æµ‹ + è¯„ä¼°
    # --------------------------------------------------
    pred = spam_classification(tags_train, sent_vec_train, sent_vec_test)
    evaluate(tags_test, pred)
    logging.info('ğŸ‰  ä»»åŠ¡å®Œæˆï¼')

    # è®°å½•è„šæœ¬ç»“æŸæ—¶é—´å¹¶æ‰“å°è€—æ—¶
    end_time = time.time()
    elapsed = end_time - start_time
    # æ ¼å¼åŒ–ä¸º æ—¶:åˆ†:ç§’
    m, s = divmod(int(elapsed), 60)
    h, m = divmod(m, 60)
    logging.info('ğŸ”” è„šæœ¬æ€»è€—æ—¶ï¼š%då°æ—¶%02dåˆ†%02dç§’', h, m, s)


if __name__ == '__main__':
    main()